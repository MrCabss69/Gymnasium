{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import deque\n",
    "\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Lambda\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from ParabolicShot import ParabolicShotEnv\n",
    "import gymnasium as gym\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "\n",
    "\n",
    "class ScaleLayer(Layer):\n",
    "    def __init__(self, scale_factor, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.scale_factor = scale_factor\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs * self.scale_factor\n",
    "\n",
    "class SoftplusLayer(Layer):\n",
    "    def __init__(self, epsilon, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.nn.softplus(inputs) + self.epsilon\n",
    "    \n",
    "    \n",
    "class DistributionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, action_dim, action_bound, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.action_dim = action_dim\n",
    "        self.action_bound = action_bound\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.mu_layer = Dense(self.action_dim, activation='tanh')\n",
    "        self.sigma_layer = Dense(self.action_dim, activation='softplus')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        mu = self.mu_layer(inputs) * self.action_bound \n",
    "        sigma = tf.clip_by_value(self.sigma_layer(inputs), 1e-6, 1e+6)\n",
    "        return mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOAgent:\n",
    "    def __init__(self, state_dim, continuous_action_dim, continuous_action_bound, actor_lr=0.001, critic_lr=0.01, network_size=[64, 64], clip_param=0.2):\n",
    "        self.state_dim = state_dim\n",
    "        self.continuous_action_dim = continuous_action_dim\n",
    "        self.continuous_action_bound = continuous_action_bound\n",
    "        self.actor_optimizer = Adam(learning_rate=actor_lr)\n",
    "        self.critic_optimizer = Adam(learning_rate=critic_lr)\n",
    "        self.network_size = network_size\n",
    "        self.clip_param = clip_param\n",
    "        self.actor = self.build_actor()\n",
    "        self.critic = self.build_critic()\n",
    "\n",
    "    def build_actor(self):\n",
    "        inputs = Input(shape=(self.state_dim,))\n",
    "        x = self.build_network(inputs, self.network_size)\n",
    "        mu, sigma = DistributionLayer(self.continuous_action_dim, self.continuous_action_bound)(x)\n",
    "        return Model(inputs=inputs, outputs=[mu, sigma])\n",
    "    \n",
    "    def build_critic(self):\n",
    "        inputs = Input(shape=(self.state_dim,))\n",
    "        x = self.build_network(inputs, self.network_size)\n",
    "        outputs = Dense(1)(x)\n",
    "        return Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    def build_network(self, inputs, layers_sizes):\n",
    "        x = inputs\n",
    "        for size in layers_sizes:\n",
    "            x = Dense(size, activation='relu', kernel_initializer='he_uniform')(x)\n",
    "        return x\n",
    "\n",
    "    def policy(self, state):\n",
    "        mu, sigma = self.actor(np.array([state]))\n",
    "        dist = tfd.TruncatedNormal(loc=mu[0], scale=sigma[0], low=-self.continuous_action_bound, high=self.continuous_action_bound)\n",
    "        action = dist.sample()\n",
    "        return action.numpy()\n",
    "\n",
    "    def train(self, replay_buffer):\n",
    "        batch_size = 64\n",
    "        if len(replay_buffer) < batch_size:\n",
    "            return None, None\n",
    "        samples = np.array(replay_buffer, dtype=object)\n",
    "        states, actions, rewards, next_states, dones = map(np.stack, zip(*samples))\n",
    "        mean_rewards = np.mean(rewards)\n",
    "        std_rewards = np.std(rewards) + 1e-8\n",
    "        rewards = (rewards - mean_rewards) / std_rewards\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            mu, sigma = self.actor(states, training=True)\n",
    "            dist = tfd.TruncatedNormal(loc=mu, scale=sigma, low=-self.continuous_action_bound, high=self.continuous_action_bound)\n",
    "            log_probs = dist.log_prob(actions)\n",
    "            log_probs = tf.reduce_sum(log_probs, axis=-1)\n",
    "            \n",
    "            critic_value = self.critic(states, training=True)\n",
    "            advantage = (rewards - tf.squeeze(critic_value))\n",
    "            advantage = (advantage - tf.reduce_mean(advantage)) / (tf.math.reduce_std(advantage) + 1e-8)\n",
    "\n",
    "            actor_loss = -tf.reduce_mean(log_probs * advantage)\n",
    "            entropy_bonus = -0.01 * tf.reduce_mean(dist.entropy())\n",
    "            total_actor_loss = actor_loss + entropy_bonus\n",
    "            \n",
    "            critic_loss = tf.reduce_mean((rewards - tf.squeeze(critic_value))**2)\n",
    "\n",
    "        critic_grads = tape.gradient(critic_loss, self.critic.trainable_variables)\n",
    "        actor_grads = tape.gradient(total_actor_loss, self.actor.trainable_variables)\n",
    "        self.critic_optimizer.apply_gradients(zip(critic_grads, self.critic.trainable_variables))\n",
    "        self.actor_optimizer.apply_gradients(zip(actor_grads, self.actor.trainable_variables))\n",
    "\n",
    "        return actor_loss, critic_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(episodes=1000):\n",
    "    env = ParabolicShotEnv(mode=None)  # Assumiendo que 'None' significa sin visualizaciÃ³n en tiempo real\n",
    "    agent = PPOAgent(env.observation_space.shape[0], 3, env.action_space.spaces['continuous'].high)\n",
    "    replay_buffer = deque(maxlen=1000)\n",
    "    \n",
    "    for episode in tqdm(range(episodes)):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.policy(state)\n",
    "            hybrid_action = {'continuous': action, 'discrete': 1 if np.random.rand() > 0.5 else 0}\n",
    "            next_state, reward, done, info = env.step(hybrid_action)\n",
    "            replay_buffer.append((state.copy(), hybrid_action['continuous'], reward, next_state.copy(), done))\n",
    "            state = next_state\n",
    "\n",
    "        if episode % 10 == 0:\n",
    "            agent.train(list(replay_buffer))\n",
    "    \n",
    "    agent.actor.save_weights('/home/jd/Documentos/CODIGO/OpenAIGym/trained/parabolic_actor.weights.h5')\n",
    "    agent.critic.save_weights('/home/jd/Documentos/CODIGO/OpenAIGym/trained/parabolic_critic.weights.h5')\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_agent():\n",
    "    env = ParabolicShotEnv()\n",
    "    agent = PPOAgent(env.observation_space.shape[0], 3, env.action_space.spaces['continuous'].high)\n",
    "    agent.actor.load_weights('/home/jd/Documentos/CODIGO/OpenAIGym/trained/parabolic_actor.weights.h5')\n",
    "    agent.critic.load_weights('/home/jd/Documentos/CODIGO/OpenAIGym/trained/parabolic_critic.weights.h5')\n",
    "    \n",
    "    for _ in tqdm(range(10)):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        while not done:\n",
    "            action = agent.policy(state)\n",
    "            hybrid_action = {'continuous': action, 'discrete': 1 if np.random.rand() > 0.5 and steps > 10 else 0}\n",
    "            print(hybrid_action)  # Debugging\n",
    "            state, reward, done, _ = env.step(hybrid_action)\n",
    "            total_reward += reward\n",
    "            steps+=1\n",
    "            env.render()\n",
    "            time.sleep(1)\n",
    "            \n",
    "\n",
    "        print(f'Total Reward: {total_reward}')\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate_agent()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
