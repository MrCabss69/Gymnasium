{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RL - DQN con Keras y Gymnasium\n",
    "################################\n",
    "# Este cuaderno Jupyter presenta una implementación del algoritmo Deep Q-Network (DQN) aplicadoaplicado en entornos de la librería Gymnasium. \n",
    "# Utilizando la librería Keras para construir y entrenar una red neuronal, exploramos cómo un agente aprender de las recompensas del entorno.\n",
    "# Creado por: [@MrCabss69]\n",
    "# Fecha de creación: Thu Mar 11 2024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constantes de configuración\n",
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplayBuffer:\n",
    "    \"\"\"Gestiona un buffer de experiencias para el aprendizaje por refuerzo.\"\"\"\n",
    "\n",
    "    def __init__(self, capacity=1000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, experience):\n",
    "        \"\"\"Añade una experiencia al buffer.\"\"\"\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Devuelve una muestra aleatoria de experiencias del buffer.\"\"\"\n",
    "        batch_size = min(batch_size, len(self.buffer))\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Devuelve el tamaño actual del buffer.\"\"\"\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape, num_actions):\n",
    "    \"\"\"Construye y devuelve un modelo de red neuronal para el agente DQN.\"\"\"\n",
    "    model = Sequential([\n",
    "        Dense(512, activation='relu', input_shape=input_shape),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(num_actions, activation='linear')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=LEARNING_RATE), loss='mse')\n",
    "    return model\n",
    "\n",
    "def epsilon_greedy(state, model, epsilon=0.1):\n",
    "    \"\"\"Selecciona una acción usando la política ε-greedy.\"\"\"\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(model.output_shape[-1])\n",
    "    else:\n",
    "        q_values = model.predict(state[np.newaxis])\n",
    "        return np.argmax(q_values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, target_model, buffer, env, episodes=1000, batch_size=64, update_freq=10):\n",
    "    \"\"\"Entrena el modelo del agente DQN.\"\"\"\n",
    "    for episode in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "\n",
    "        while not done:\n",
    "            action = epsilon_greedy(state, model)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            score += reward\n",
    "            buffer.add((state, action, reward, next_state, done))\n",
    "            state = next_state\n",
    "\n",
    "            if len(buffer) >= batch_size:\n",
    "                samples = buffer.sample(batch_size)\n",
    "                states, actions, rewards, next_states, dones = map(np.array, zip(*samples))\n",
    "                q_values_next = target_model.predict(next_states)\n",
    "                q_values_next[dones] = 0\n",
    "                targets = rewards + GAMMA * np.amax(q_values_next, axis=1)\n",
    "                q_values = model.predict(states)\n",
    "                q_values[range(batch_size), actions] = targets\n",
    "                model.fit(states, q_values, epochs=1, verbose=0)\n",
    "\n",
    "            if episode % update_freq == 0:\n",
    "                target_model.set_weights(model.get_weights())\n",
    "\n",
    "        print(f\"Episode: {episode + 1}/{episodes}, Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "num_actions = env.action_space.n\n",
    "state_shape = env.observation_space.shape\n",
    "\n",
    "model = create_model((state_shape[0],), num_actions)\n",
    "target_model = create_model((state_shape[0],), num_actions)\n",
    "buffer = ExperienceReplayBuffer(capacity=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model, target_model, buffer, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def plot_weights(model, layer_index, annot=True, title=\"Weights Visualization\"):\n",
    "    weights = model.layers[layer_index].get_weights()[0]\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    sns.heatmap(weights, annot=annot, fmt=\".2f\", cmap='viridis')\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_weights(model,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('/trained/dqn_1k.h5')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
