{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-12 01:48:22.696127: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-12 01:48:22.739897: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-12 01:48:22.740607: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-12 01:48:23.410496: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import random\n",
    "from collections import deque\n",
    "from ParabolicShot import ParabolicShotEnv\n",
    "\n",
    "# Hiperparámetros\n",
    "GAMMA = 0.99\n",
    "LEARNING_RATE_ACTOR = 0.0001\n",
    "LEARNING_RATE_CRITIC = 0.001\n",
    "TAU = 0.001  # Para actualización suave de las redes objetivo\n",
    "MEMORY_CAPACITY = 100000\n",
    "BATCH_SIZE = 64\n",
    "EXPLORATION_NOISE = 0.1\n",
    "\n",
    "# Definición del Actor\n",
    "def create_actor(state_dim, action_dim, action_bound):\n",
    "    inputs = Input(shape=(state_dim,))\n",
    "    out = Dense(400, activation=\"relu\")(inputs)\n",
    "    out = Dense(300, activation=\"relu\")(out)\n",
    "    outputs = Dense(action_dim, activation=\"tanh\")(out)\n",
    "    scaled_outputs = outputs * action_bound  # Asume que el límite de acción es simétrico\n",
    "    model = Model(inputs, scaled_outputs)\n",
    "    model.compile(optimizer=Adam(learning_rate=LEARNING_RATE_ACTOR))\n",
    "    return model\n",
    "\n",
    "# Definición del Crítico\n",
    "def create_critic(state_dim, action_dim):\n",
    "    state_inputs = Input(shape=(state_dim,))\n",
    "    action_inputs = Input(shape=(action_dim,))\n",
    "    concatenated = Concatenate()([state_inputs, action_inputs])\n",
    "    out = Dense(400, activation=\"relu\")(concatenated)\n",
    "    out = Dense(300, activation=\"relu\")(out)\n",
    "    outputs = Dense(1, activation=\"linear\")(out)\n",
    "    model = Model([state_inputs, action_inputs], outputs)\n",
    "    model.compile(optimizer=Adam(learning_rate=LEARNING_RATE_CRITIC), loss='mse')\n",
    "    return model\n",
    "\n",
    "# Memoria de repetición de experiencia\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# Ruido de exploración\n",
    "class OUNoise:\n",
    "    def __init__(self, action_dimension, scale=0.1, mu=0, theta=0.15, sigma=0.2):\n",
    "        self.action_dimension = action_dimension\n",
    "        self.scale = scale\n",
    "        self.mu = mu\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.state = np.ones(self.action_dimension) * self.mu\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = np.ones(self.action_dimension) * self.mu\n",
    "    \n",
    "    def noise(self):\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(len(x))\n",
    "        self.state = x + dx\n",
    "        return self.state * self.scale\n",
    "\n",
    "# Agente DDPG\n",
    "class DDPGAgent:\n",
    "    def __init__(self, state_dim, action_dim, action_bound):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.action_bound = action_bound\n",
    "        \n",
    "        self.actor = create_actor(self.state_dim, self.action_dim, self.action_bound)\n",
    "        self.critic = create_critic(self.state_dim, self.action_dim)\n",
    "        self.target_actor = create_actor(self.state_dim, self.action_dim, self.action_bound)\n",
    "        self.target_critic = create_critic(self.state_dim, self.action_dim)\n",
    "        \n",
    "        # Inicializar los modelos objetivo\n",
    "        self.target_actor.set_weights(self.actor.get_weights())\n",
    "        self.target_critic.set_weights(self.critic.get_weights())\n",
    "        \n",
    "        self.buffer = ReplayBuffer(MEMORY_CAPACITY)\n",
    "        self.noise = OUNoise(self.action_dim)\n",
    "\n",
    "    def policy(self, state):\n",
    "        state = np.reshape(state, [1, -1])  # Asegúrate de que state es un array 2D con la forma correcta\n",
    "        action = self.actor.predict(state)[0]\n",
    "        return action + self.noise.noise()\n",
    "\n",
    "    def train(self):\n",
    "        if len(self.buffer) < BATCH_SIZE:\n",
    "            return\n",
    "        \n",
    "        samples = self.buffer.sample(BATCH_SIZE)\n",
    "        states, actions, rewards, next_states, dones = map(np.array, zip(*samples))\n",
    "        \n",
    "        # Preparación de la actualización del crítico\n",
    "        target_actions = self.target_actor.predict(next_states)\n",
    "        future_rewards = self.target_critic.predict([next_states, target_actions]).flatten()\n",
    "        q_values = rewards + GAMMA * future_rewards * (1 - dones)\n",
    "        self.critic.train_on_batch([states, actions], q_values.reshape(-1, 1))\n",
    "        \n",
    "        # Preparación de la actualización del actor usando GradientTape para calcular los gradientes\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(self.actor.trainable_variables)\n",
    "            actions_pred = self.actor(states)\n",
    "            q_values_pred = self.critic([states, actions_pred])\n",
    "            actor_loss = -tf.reduce_mean(q_values_pred)\n",
    "\n",
    "        actor_grads = tape.gradient(actor_loss, self.actor.trainable_variables)\n",
    "        self.actor.optimizer.apply_gradients(zip(actor_grads, self.actor.trainable_variables))\n",
    "        \n",
    "        # Actualización de las redes objetivo\n",
    "        self.update_target(self.target_actor.variables, self.actor.variables, TAU)\n",
    "        self.update_target(self.target_critic.variables, self.critic.variables, TAU)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def update_target(target_weights, weights, tau):\n",
    "        for (a, b) in zip(target_weights, weights):\n",
    "            a.assign(b * tau + a * (1 - tau))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jd/.local/lib/python3.11/site-packages/gym/spaces/box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/jd/.local/lib/python3.11/site-packages/keras/engine/training.py\", line 2169, in predict_function  *\n        return step_function(self, iterator)\n    File \"/home/jd/.local/lib/python3.11/site-packages/keras/engine/training.py\", line 2155, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/jd/.local/lib/python3.11/site-packages/keras/engine/training.py\", line 2143, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/home/jd/.local/lib/python3.11/site-packages/keras/engine/training.py\", line 2111, in predict_step\n        return self(x, training=False)\n    File \"/home/jd/.local/lib/python3.11/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/jd/.local/lib/python3.11/site-packages/keras/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model\" is incompatible with the layer: expected shape=(None, 7), found shape=(None, 5)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m---> 19\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     next_state, reward, done, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     21\u001b[0m     agent\u001b[38;5;241m.\u001b[39mremember(state, action, reward, next_state, done)  \u001b[38;5;66;03m# Guardar en memoria de repetición\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 97\u001b[0m, in \u001b[0;36mDDPGAgent.policy\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpolicy\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[1;32m     96\u001b[0m     state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(state, [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])  \u001b[38;5;66;03m# Asegúrate de que state es un array 2D con la forma correcta\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m action \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnoise\u001b[38;5;241m.\u001b[39mnoise()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file41zjhf5q.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/jd/.local/lib/python3.11/site-packages/keras/engine/training.py\", line 2169, in predict_function  *\n        return step_function(self, iterator)\n    File \"/home/jd/.local/lib/python3.11/site-packages/keras/engine/training.py\", line 2155, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/jd/.local/lib/python3.11/site-packages/keras/engine/training.py\", line 2143, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/home/jd/.local/lib/python3.11/site-packages/keras/engine/training.py\", line 2111, in predict_step\n        return self(x, training=False)\n    File \"/home/jd/.local/lib/python3.11/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/jd/.local/lib/python3.11/site-packages/keras/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model\" is incompatible with the layer: expected shape=(None, 7), found shape=(None, 5)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Crea el entorno\n",
    "env = ParabolicShotEnv()\n",
    "episodes = 100  # Define el número de episodios de entrenamiento\n",
    "\n",
    "# Instancia el agente\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "action_bound = env.action_space.high[0]  # Asume acción simétrica alrededor de cero\n",
    "\n",
    "agent = DDPGAgent(state_dim, action_dim, action_bound)\n",
    "\n",
    "# Proceso de entrenamiento\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = agent.policy(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        agent.remember(state, action, reward, next_state, done)  # Guardar en memoria de repetición\n",
    "        agent.train()  # Entrenamiento del agente\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "        env.render()  # Visualiza el entorno\n",
    "        time.sleep(1)  # Pausa entre pasos para visualización\n",
    "\n",
    "    print(f'Episode: {episode+1}, Reward: {episode_reward}')\n",
    "\n",
    "    if (episode + 1) % 10 == 0:  # Actualizar el modelo objetivo cada 10 episodios\n",
    "        agent.update_target_model()\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
