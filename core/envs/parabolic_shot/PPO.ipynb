{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import deque\n",
    "\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Lambda\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from ParabolicShot import ParabolicShotEnv\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "\n",
    "\n",
    "class ScaleLayer(Layer):\n",
    "    def __init__(self, scale_factor, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.scale_factor = scale_factor\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs * self.scale_factor\n",
    "\n",
    "class SoftplusLayer(Layer):\n",
    "    def __init__(self, epsilon, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.nn.softplus(inputs) + self.epsilon\n",
    "    \n",
    "    \n",
    "class DistributionLayer(Layer):\n",
    "    def __init__(self, action_dim, continuous_action_bound, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.action_dim = action_dim\n",
    "        self.continuous_action_bound = continuous_action_bound\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.mu_layer = Dense(self.action_dim, activation='tanh')\n",
    "        self.sigma_layer = Dense(self.action_dim, activation='softplus')\n",
    "        self.scale_layer = ScaleLayer(self.continuous_action_bound)\n",
    "        self.adjust_sigma_layer = SoftplusLayer(0.0001)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        mu = self.mu_layer(inputs)\n",
    "        sigma = self.sigma_layer(inputs)\n",
    "        mu_scaled = self.scale_layer(mu)\n",
    "        sigma_adjusted = self.adjust_sigma_layer(sigma)\n",
    "        return mu_scaled, sigma_adjusted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOAgent:\n",
    "    def __init__(self, state_dim, continuous_action_dim, continuous_action_bound):\n",
    "        self.state_dim = state_dim\n",
    "        self.continuous_action_dim = continuous_action_dim\n",
    "        self.continuous_action_bound = continuous_action_bound\n",
    "        self.actor_optimizer = Adam(learning_rate=0.001)\n",
    "        self.critic_optimizer = Adam(learning_rate=0.01)\n",
    "        self.actor = self.build_actor()\n",
    "        self.critic = self.build_critic()\n",
    "\n",
    "    def build_actor(self):\n",
    "        inputs = Input(shape=(self.state_dim,))\n",
    "        x = Dense(64, activation='relu', kernel_initializer='he_uniform')(inputs)\n",
    "        x = Dense(64, activation='relu', kernel_initializer='he_uniform')(x)\n",
    "        mu, sigma = DistributionLayer(self.continuous_action_dim, self.continuous_action_bound)(x)\n",
    "        return Model(inputs=inputs, outputs=[mu, sigma])\n",
    "\n",
    "    \n",
    "    def policy(self, state):\n",
    "        mu, sigma = self.actor(np.array([state]))\n",
    "        dist = tfd.Normal(loc=mu[0], scale=sigma[0])  # Asegura usar el primer (y único) batch\n",
    "        action = dist.sample()\n",
    "        return np.clip(action, -self.continuous_action_bound, self.continuous_action_bound)\n",
    "\n",
    "\n",
    "    def build_critic(self):\n",
    "        inputs = Input(shape=(self.state_dim,))\n",
    "        x = Dense(64, activation='relu', kernel_initializer='he_uniform')(inputs)\n",
    "        x = Dense(64, activation='relu', kernel_initializer='he_uniform')(x)\n",
    "        outputs = Dense(1, activation='linear')(x)\n",
    "        return Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    def train(self, replay_buffer):\n",
    "        batch_size = 64\n",
    "        if len(replay_buffer) < batch_size:\n",
    "            return None, None\n",
    "\n",
    "        samples = np.array(replay_buffer, dtype=object)\n",
    "        states, continuous_actions, rewards, next_states, dones = map(np.stack, zip(*samples))\n",
    "        rewards = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-8)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            critic_value = self.critic(states, training=True)\n",
    "            critic_loss = MeanSquaredError()(rewards, tf.squeeze(critic_value))\n",
    "\n",
    "        critic_grads = tape.gradient(critic_loss, self.critic.trainable_variables)\n",
    "        self.critic_optimizer.apply_gradients(zip(critic_grads, self.critic.trainable_variables))\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            mu, sigma = self.actor(states, training=True)\n",
    "            dist = tfd.Normal(loc=mu, scale=sigma)\n",
    "            log_probs = dist.log_prob(continuous_actions)\n",
    "            log_probs = tf.reduce_sum(log_probs, axis=-1)  # Sumar a lo largo de la dimensión de acción\n",
    "\n",
    "            critic_value = self.critic(states)\n",
    "            advantage = (rewards - tf.squeeze(critic_value))\n",
    "            advantage = (advantage - tf.reduce_mean(advantage)) / (tf.math.reduce_std(advantage) + 1e-8)\n",
    "\n",
    "            actor_loss = -tf.reduce_mean(log_probs * advantage)\n",
    "            entropy_bonus = -0.01 * tf.reduce_mean(dist.entropy())\n",
    "            total_actor_loss = actor_loss + entropy_bonus\n",
    "\n",
    "        actor_grads = tape.gradient(total_actor_loss, self.actor.trainable_variables)\n",
    "        actor_grads = [tf.clip_by_norm(g, 1.0) if g is not None else None for g in actor_grads]\n",
    "        self.actor_optimizer.apply_gradients(zip(actor_grads, self.actor.trainable_variables))\n",
    "\n",
    "        return actor_loss, critic_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(episodes=110):\n",
    "    env = ParabolicShotEnv()\n",
    "    agent = PPOAgent(env.observation_space.shape[0], 3, env.action_space.spaces['continuous'].high)  # Ajustar según la definición correcta de dimensiones y límites\n",
    "    replay_buffer = deque(maxlen=1000)\n",
    "\n",
    "    for episode in tqdm(range(episodes)):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            action = agent.policy(state)\n",
    "            hybrid_action = {'continuous': action, 'discrete': 1 if np.random.rand() > 0.5 else 0}  # Ejemplo de cómo seleccionar acción discreta\n",
    "            next_state, reward, done, info = env.step(hybrid_action)\n",
    "            replay_buffer.append((state.copy(), hybrid_action['continuous'], reward, next_state.copy(), done))\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "        if episode % 10 == 0:\n",
    "            actor_loss, critic_loss = agent.train(list(replay_buffer))\n",
    "            if episode % 100 == 0 and actor_loss is not None:  # Cambia 100 por cualquier otro número dependiendo de la frecuencia deseada\n",
    "                print(f'Episode: {episode+1}, Total Reward: {total_reward}, Actor Loss: {actor_loss.numpy()}, Critic Loss: {critic_loss.numpy()}')\n",
    "\n",
    "    agent.actor.save_weights('/home/jd/Documentos/CODIGO/OpenAIGym/trained/parabolic_actor.weights.h5')\n",
    "    agent.critic.save_weights('/home/jd/Documentos/CODIGO/OpenAIGym/trained/parabolic_critic.weights.h5')\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_agent():\n",
    "    env = ParabolicShotEnv()\n",
    "    agent = PPOAgent(env.observation_space.shape[0], 3, env.action_space.spaces['continuous'].high)\n",
    "    agent.actor.load_weights('/home/jd/Documentos/CODIGO/OpenAIGym/trained/parabolic_actor.weights.h5')\n",
    "    agent.critic.load_weights('/home/jd/Documentos/CODIGO/OpenAIGym/trained/parabolic_critic.weights.h5')\n",
    "    \n",
    "    for _ in tqdm(range(10)):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            action = agent.policy(state)\n",
    "            hybrid_action = {'continuous': action, 'discrete': 1 if np.random.rand() > 0.5 else 0}\n",
    "            print(hybrid_action)  # Debugging\n",
    "            state, reward, done, _ = env.step(hybrid_action)\n",
    "            total_reward += reward\n",
    "            env.render(mode='human')\n",
    "            time.sleep(1)\n",
    "\n",
    "        print(f'Total Reward: {total_reward}')\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate_agent()\n",
    "train_agent(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulate_agent()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
